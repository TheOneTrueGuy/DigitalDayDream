{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Digital daydreaming in latent space with CLIP Interogator and Stable Diffusion"
      ],
      "metadata": {
        "id": "o88Utuba8NiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "INL8Ds7z8dO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers==0.2.4\n",
        "#!pip install -qq -U diffusers==0.3.0"
      ],
      "metadata": {
        "id": "VkOhQLDqI5fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def setup():\n",
        "    install_cmds = [\n",
        "        #['pip', 'install', '-e', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],\n",
        "        ['pip', 'install', '-e', 'git+https://github.com/MirageML/BLIP.git@main#egg=blip'],#  https://github.com/pharmapsychotic/BLIP.git@lib#egg=blip'],\n",
        "        ['git', 'clone', 'https://github.com/pharmapsychotic/clip-interrogator.git'],\n",
        "        ['pip', 'install', 'open_clip_torch', 'ftfy', 'gradio', 'regex', 'tqdm', 'transformers==4.24.0', 'timm', 'fairscale', 'requests']        \n",
        "    ]\n",
        "    for cmd in install_cmds:\n",
        "        print(subprocess.run(cmd, stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "setup()\n",
        "\n",
        "import sys\n",
        "sys.path.append('src/blip')\n",
        "sys.path.append('src/clip')\n",
        "sys.path.append('clip-interrogator')\n",
        "\n",
        "import gradio as gr\n",
        "from clip_interrogator import Config, Interrogator\n",
        "\n",
        "ci = Interrogator(Config())\n",
        "\n",
        "def inference(image, mode):\n",
        "    image = image.convert('RGB')\n",
        "    if mode == 'best':\n",
        "        return ci.interrogate(image)\n",
        "    elif mode == 'classic':\n",
        "        return ci.interrogate_classic(image)\n",
        "    else:\n",
        "        return ci.interrogate_fast(image)\n"
      ],
      "metadata": {
        "id": "EnIEAOOrEPIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir spun\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/stub_materials/figor.png /content/st.png #your seed image\n",
        "!cp /content/drive/MyDrive/stub_materials/image_to_image.py /content/"
      ],
      "metadata": {
        "id": "arj7-WnR_vCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# don't forget:\n",
        "# huggingface-cli login\n",
        "# I always keep my token # here\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from image_to_image import StableDiffusionImg2ImgPipeline, preprocess\n",
        "#from diffusers.models import AutoencoderKL\n",
        "class FakeSafety():\n",
        "    def __call__(self, clip_input, images):\n",
        "        return (images, False)\n",
        "from torch import autocast\n",
        "rond=np.random.randint(7,100000000)\n",
        "generator = torch.Generator(\"cuda\").manual_seed(rond)\n",
        "print( \"random seed \" + str(rond))\n",
        "device = \"cuda\"\n",
        "#vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\")\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    #vae=vae,\n",
        "    revision=\"fp16\", \n",
        "    torch_dtype=torch.float16,\n",
        "    use_auth_token=True\n",
        ").to(device)\n",
        "\n",
        "pipe.safety_checker = FakeSafety()\n",
        "file1 = open(\"dialog1.txt\", \"a\")\n",
        "for xn in range(0,250,1):\n",
        "  im= Image.open(\"st.png\").convert(\"RGB\")\n",
        "  im=im.resize((512,512))\n",
        "  cliprompt=ci.interrogate_fast(im)\n",
        "  print(cliprompt)\n",
        "  file1.write(cliprompt+\"\\n\")\n",
        "  init_image = preprocess(im)\n",
        "  with autocast(\"cuda\"):\n",
        "    image = pipe(cliprompt, init_image=init_image, strength=0.65, guidance_scale=7.0, num_inference_steps=20, generator=generator)[\"sample\"][0]  # image\n",
        "  image.save(\"st.png\")\n",
        "  image.save(\"spun2/\"+str(xn).zfill(4)+\".png\")\n",
        "  os.system(\"bash 1julien.sh st.png zi\")\n",
        "file1.close()\n",
        "!cp dialog1.txt spun2/\n",
        "!zip -r 11_29_22_spun2.zip spun2/ \n",
        "!cp 11_29_22_spun2.zip /content/drive/MyDrive/product"
      ],
      "metadata": {
        "id": "7Vin6RgA85gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "r1QN-3NUFVtz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}